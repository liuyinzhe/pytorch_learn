
t(x) =m(x+2)**2



叶子节点x,自变量；t 因变量， m表示取平均值；
    t 函数的变化 dt; x的变化 dx;
    d 导数
    问题：当 t 发生变化 dt, 那么 dx 的变化多大
    问题可转化为 求dt/dx的导数
    
    pytoch 中提供了.backward() 自动求导
    t.backward()
    

神经网络过程(意义)：

    y = ax + b  # 函数参数，a b，自变量x ;暂时定义 t(x)函数 内容就是 t(x) ax+b
    相对真实值的因变量y， a,b 作为输入对y 的影响，可解释为平均损失函数L(损失loss)；
    
    i点预测y = a* i点x + b
    
    平均损失函数 = 1/N * sum(i点真实y -i点预测y)

    最终目的则是找到 a,b 组合中可以达到 所有 最小损失
    
    当我们输入连续的多组a,b 时， 损失函数的变化 可以用 y对 a，d的导数(多变量也称偏导)，机器学习中称为梯度(导数)，用导数来判断
    继而 其实就是寻找 导数 最低，随时函数变化最小的 参数 a,b
    
    

定义为自动微分变量  #requires_grad=True

 三个属性:
 
    data:存储张量
    
    grad:梯度，就是导数数值,取值方式:x.grad.data
    
    grad_fn:记录计算图的上一步骤，如x+2  结果z，z.grad_fn 内容是AddBackward0 at 0x内存地址
    
名词：

    反向传播，根据计算图中，grad_fn 记录上一步计算的形式，进行求梯度操作

    求梯度： 高等数学中的求导运算

    梯度信息：导数数值
    
    叶子节点： 自变量x，输入
    
    学习率： 
    
    
另外入门视频(主要理解过程作用，公式可以忽略)：
[5分钟学深度学习](https://space.bilibili.com/168709400/channel/seriesdetail?sid=2430388)

    梯度下降
    反向传播
    激活函数
